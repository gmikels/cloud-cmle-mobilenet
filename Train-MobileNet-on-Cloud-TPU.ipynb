{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Environment Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Cell Magic to Set Environment Variables\n",
    "%env PROJECT=gmikles-test-codelab5\n",
    "%env YOUR_GCS_BUCKET=gmikels-test-codelab5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install All Dependencies**\n",
    "\n",
    "This tutorial requires the use of Tensorflow Object Detection. \n",
    "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python Dependencies for Tensorflow Object Detection\n",
    "!pip3 install --user Cython\n",
    "!pip3 install --user contextlib2\n",
    "!pip3 install --user pillow\n",
    "!pip3 install --user lxml\n",
    "!pip3 install --user jupyter\n",
    "!pip3 install --user matplotlib\n",
    "!pip3 install --user tensorflow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Required Source from Github\n",
    "%cd /content/datalab\n",
    "!git clone https://github.com/cocodataset/cocoapi.git\n",
    "!git clone https://github.com/tensorflow/tensorflow.git\n",
    "!git clone https://github.com/tensorflow/models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO API Installation\n",
    "%cd /content/datalab/cocoapi/PythonAPI\n",
    "!make\n",
    "!cp -r pycocotools /content/datalab/models/research/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual protobuf-compiler installation and usage\n",
    "%cd /content/datalab/models/research\n",
    "!wget -O protobuf.zip https://github.com/google/protobuf/releases/download/v3.0.0/protoc-3.0.0-linux-x86_64.zip\n",
    "!unzip protobuf.zip\n",
    "!./bin/protoc object_detection/protos/*.proto --python_out=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Libraries to PYTHONPATH\n",
    "%cd /content/datalab/models/research\n",
    "%env PYTHONPATH=$PYTHONPATH:/content/datalab/models/research:/content/datalab/models/research/slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Tensorflow Object Detection**\n",
    "\n",
    "You should see output similar to this after running the next command.\n",
    "\n",
    "\n",
    "![alt text](https://screenshot.googleplex.com/NQ2CpJVEoGJ.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the Installation of Tensorflow Object Detection\n",
    "!python3 -m object_detection.builders.model_builder_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up the Dataset**\n",
    "\n",
    "To keep things simple, we’ll use a small pet breeds dataset. This dataset includes around 7,400 images — ~200 images for 37 different cat and dog breeds. Each image has an associated annotations file, which includes the bounding box coordinates where the specific pet is located in the image. We can’t feed these images and annotations directly to our model; we need to convert them into a format our model can understand. For this we’ll use the TFRecord format.\n",
    "\n",
    "The Oxford-IIIT Pet data set is located: http://www.robots.ox.ac.uk/~vgg/data/pets/. To download, extract and convert it to TFRecrods, run the following commands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and Extract Images\n",
    "%cd /content/datalab/models/research\n",
    "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
    "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
    "!tar -xvf annotations.tar.gz\n",
    "!tar -xvf images.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TFRecords from Downloaded Images. Note that some examples may be ignored.\n",
    "%cd /content/datalab/models/research\n",
    "!mkdir /content/datalab/preprocess_output\n",
    "!python3 -m object_detection.dataset_tools.create_pet_tf_record\\\n",
    "    --label_map_path=/content/datalab/models/research/object_detection/data/pet_label_map.pbtxt \\\n",
    "    --data_dir=/content/datalab/models/research \\\n",
    "    --output_dir=/content/datalab/preprocess_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Data Files to Google Cloud Storage \n",
    "!gsutil -m cp -r /content/datalab/preprocess_output/* gs://$YOUR_GCS_BUCKET/data/\n",
    "!gsutil cp /content/datalab/models/research/object_detection/data/pet_label_map.pbtxt gs://$YOUR_GCS_BUCKET/data/pet_label_map.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the SSD MobileNet Checkpoint for Transfer Learning**\n",
    "\n",
    "Training a model to recognize pet breeds from scratch would take thousands of training images for each pet breed and hours or days of training time. To speed this up, we can make use of transfer learning — a process where we take the weights of a model that has already been trained on lots of data to perform a similar task, and then train the model on our own data, fine tuning the layers from the pre-trained model.\n",
    "\n",
    "There are many models we can use that have been trained to recognize a wide variety of objects in images. We can use the checkpoints from these trained models and then apply them to our custom object detection task. This works because, to a machine, the task of identifying the pixels in an image that contain basic objects like tables, chairs, or cats isn’t so different from identifying the pixels in an image that contain specific pet breeds.\n",
    "\n",
    "For this example we’ll use SSD with MobileNet, an object detection model optimized for inference on mobile. First, download and extract the latest MobileNet checkpoint that’s been pretrained on the COCO dataset. To see a list of all the models that the Object Detection API supports, check out the model zoo: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md. Once you’ve extracted the checkpoint, copy the 3 files into your GCS bucket. Run the commands below to download the checkpoint and copy it into your bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Model Checkpoints\n",
    "%cd /tmp\n",
    "!curl -O http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03.tar.gz\n",
    "!tar xzf ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03.tar.gz\n",
    "!gsutil cp /tmp/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03/model.ckpt.* gs://$YOUR_GCS_BUCKET/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training a Quantized Model with Cloud TPU on Cloud Machine Learning Engine**\n",
    "\n",
    "Machine learning models have two distinct computational components: training and inference. In this example, we’re making use of Cloud TPUs to accelerate training. There are a few lines in the config file that relate specifically to TPU training. We can use a larger batch size when training on TPUs since they make it easier to handle large datasets (when experimenting with batch size on your own dataset, make sure to use multiples of 8 since data needs to be divided evenly for each of the 8 TPU cores). With a larger batch size for our model, we can reduce the number of training steps (in this example we use 2000). The focal loss function we use for this training job, defined in the following lines in the config, is also a great fit for TPUs.\n",
    "\n",
    "![alt text](https://screenshot.googleplex.com/LkdPseifqst.png)\n",
    "\n",
    "This loss function computes loss for every example in the dataset and then reweights them, assigning more relative weight to hard, misclassified examples. This logic is better suited for TPUs than the hard example mining operation used in other training jobs. You can read more about focal loss here: https://arxiv.org/abs/1708.02002.\n",
    "\n",
    "Recall from above that the process of initializing a pre-trained model checkpoint and then adding our own training data is called transfer learning. The following lines in the config tell our model that we’ll be doing transfer learning for object detection, starting from a pre-trained checkpoint.\n",
    "\n",
    "![alt text](https://screenshot.googleplex.com/OuJ9oNqmaxr.png)\n",
    "\n",
    "We also need to consider how our model will be used after it’s been trained. Let’s say our pet detector becomes a global hit, used by animal lovers and pet stores everywhere. We need a scalable way to handle these inference requests with low latency. The output of a machine learning model is a binary file containing the trained weights of our model — these files are often quite large, but since we’ll be serving this model directly on a mobile device we’ll need to make it as small as possible.\n",
    "\n",
    "This is where model quantization comes in. Quantization compresses the weights and activations in our model to an 8-bit fixed point representation. The following lines in our config file will generate a quantized model:\n",
    "\n",
    "![alt text](https://screenshot.googleplex.com/RQVtBcvFWxo.png)\n",
    "\n",
    "We will download this premade config but take time to examine the contents which will be print after running the commands below.\n",
    "\n",
    "To tell ML Engine where to find our training and test files and model checkpoint, you’ll need to update a few lines in the config file we’ve created for you to point to your bucket. The code below will do this for us.\n",
    "\n",
    "We will download this premade config but take time to examine the contents which will be print after running the commands below. The config file will be uploaded to Google Cloud Storage in the command below for use with Cloud Machine Learning Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sed command is used to replace YOUR-STORAGE-BUCKET \n",
    "# in the config file with the name of your GCS Bucket\n",
    "#!git clone .config\n",
    "!sed -i 's@YOUR-STORAGE-BUCKET@'\"$YOUR_GCS_BUCKET\"'@g' /content/datalab/cloud-cmle-mobilenet/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config\n",
    "!cat /content/datalab/cloud-cmle-mobilenet/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config\n",
    "!gsutil cp /content/datalab/cloud-cmle-mobilenet/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config gs://$YOUR_GCS_BUCKET/data/pipeline.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we kick off our training job on Cloud ML Engine, we need to package the Object Detection API, pycocotools, and TF Slim. We can do that with the following command (run this from the research/ directory, and note that the parentheses are part of the command):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Files for Training on Cloud Machine Learning Engine\n",
    "%cd /content/datalab/models/research\n",
    "!bash object_detection/dataset_tools/create_pycocotools_package.sh /tmp/pycocotools\n",
    "!python setup.py sdist\n",
    "!(cd slim && python setup.py sdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start a Training Job on Cloud Machine Learning Engine**\n",
    "\n",
    "We’re ready to train our model! To kick off training, run the following gcloud command.\n",
    "\n",
    "Note that if you receive an error saying that no Cloud TPUs are available, we recommend simply trying again in a different zone (Cloud TPUs are currently available in us-central1-b, us-central1-c, europe-west4-a, and asia-east1-c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit Training Job\n",
    "%cd /content/datalab/models/research\n",
    "!gcloud ml-engine jobs submit training `whoami`_object_detection_`date +%s` \\\n",
    "--job-dir=gs://$YOUR_GCS_BUCKET/train \\\n",
    "--packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz \\\n",
    "--module-name object_detection.model_tpu_main \\\n",
    "--python-version 3.5 \\\n",
    "--runtime-version 1.9 \\\n",
    "--scale-tier BASIC_TPU \\\n",
    "--region us-central1 \\\n",
    "-- \\\n",
    "--model_dir=gs://$YOUR_GCS_BUCKET/train \\\n",
    "--tpu_zone us-central1 \\\n",
    "--pipeline_config_path=gs://$YOUR_GCS_BUCKET/data/pipeline.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the submitted job in the Google Cloud Console by navigating to **ML Engine > Jobs**. We will continue to the next section without waiting for the job to finish. It will take about 30 minutes.\n",
    "\n",
    "![alt text](https://screenshot.googleplex.com/PNL97SifydR.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install the Bazel Build Tool**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Bazel Installer\n",
    "%cd /content/datalab\n",
    "!wget https://github.com/bazelbuild/bazel/releases/download/0.19.1/bazel-0.19.1-installer-linux-x86_64.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the installer\n",
    "%cd /content/datalab\n",
    "!chmod +x bazel-0.19.1-installer-linux-x86_64.sh\n",
    "!./bazel-0.19.1-installer-linux-x86_64.sh --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting the Model to TFLite**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are not waiting on the model to finish training, let's the model checkpoints from a model that was previously trained. These checkpoints were included in a git repo we downloaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Environment Variables\n",
    "CONFIG_FILE=\"gs://$YOUR_GCS_BUCKET/data/pipeline.config\"\n",
    "CHECKPOINT_PATH=\"/content/datalab/cloud-cmle-mobilenet/model_checkpoints/model.ckpt-2000\"\n",
    "OUTPUT_DIR=\"/content/datalab/frozen_graph\"\n",
    "!echo {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze Tensorflow Graph\n",
    "%env PYTHONPATH=$PYTHONPATH:/content/datalab/models/research:/content/datalab/models/research/slim\n",
    "%cd /content/datalab/models/research\n",
    "!python3 -m object_detection.export_tflite_ssd_graph \\\n",
    "--pipeline_config_path={CONFIG_FILE} \\\n",
    "--trained_checkpoint_prefix={CHECKPOINT_PATH} \\\n",
    "--output_directory={OUTPUT_DIR} \\\n",
    "--add_postprocessing_op=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "%cd /content/datalab/tensorflow\n",
    "!/content/bin/bazel build tensorflow/python/tools:freeze_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bazel-bin/tensorflow/python/tools/freeze_graph \\\n",
    "--input_graph=/tmp/model/my_graph.pb \\\n",
    "--input_checkpoint=/tmp/model/model.ckpt-1000 \\\n",
    "--output_graph=/tmp/frozen_graph.pb \\\n",
    "--output_node_names=output_node \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!toco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TFLite Model File for Mobile Deployment\n",
    "# This command will take about 10 minutes to complete on an 8 core machine.\n",
    "%cd /content/datalab/tensorflow\n",
    "!/content/bin/bazel run -c opt tensorflow/lite/toco:toco -- \\\n",
    "--input_file={OUTPUT_DIR}/tflite_graph.pb \\\n",
    "--output_file=/content/datalab/detect.tflite \\\n",
    "--input_shapes=1,300,300,3 \\\n",
    "--input_arrays=normalized_input_image_tensor \\\n",
    "--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  \\\n",
    "--inference_type=QUANTIZED_UINT8 \\\n",
    "--mean_values=128 \\\n",
    "--std_values=128 \\\n",
    "--change_concat_input_ranges=false \\\n",
    "--allow_custom_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "%cd /content/datalab/tensorflow\n",
    "!/content/bin/bazel run tensorflow/lite/tools:visualize -- /content/datalab/detect.tflite model_viz.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "interpreter = tf.contrib.lite.Interpreter(model_path=\"/content/datalab/detect.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 freeze | grep tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user tensorflow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "%cd /content/datalab/models/research\n",
    "from object_detection.utils import ops as utils_ops\n",
    "%cd /content/datalab/models/research/object_detection\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_util\n",
    "\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import tarfile\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What model to download.\n",
    "MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "#PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "PATH_TO_FROZEN_GRAPH = '/content/datalab/frozen_graph/tflite_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "#PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
    "PATH_TO_LABELS = '/content/datalab/frozen_graph/tflite_graph.pbtxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opener = urllib.request.URLopener()\n",
    "opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "tar_file = tarfile.open(MODEL_FILE)\n",
    "for file in tar_file.getmembers():\n",
    "  file_name = os.path.basename(file.name)\n",
    "  if 'frozen_inference_graph.pb' in file_name:\n",
    "    tar_file.extract(file, os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of simplicity we will use only 2 images:\n",
    "# image1.jpg\n",
    "# image2.jpg\n",
    "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
    "PATH_TO_TEST_IMAGES_DIR = '/content/datalab/images'\n",
    "TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 3) ]\n",
    "\n",
    "# Size, in inches, of the output images.\n",
    "IMAGE_SIZE = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image, graph):\n",
    "  with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      # Get handles to input and output tensors\n",
    "      ops = tf.get_default_graph().get_operations()\n",
    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "      tensor_dict = {}\n",
    "      for key in [\n",
    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
    "          'detection_classes', 'detection_masks'\n",
    "      ]:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "              tensor_name)\n",
    "      if 'detection_masks' in tensor_dict:\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "      # Run inference\n",
    "      output_dict = sess.run(tensor_dict,\n",
    "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "      output_dict['detection_classes'] = output_dict[\n",
    "          'detection_classes'][0].astype(np.uint8)\n",
    "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "      if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in TEST_IMAGE_PATHS:\n",
    "  image = Image.open(image_path)\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = load_image_into_numpy_array(image)\n",
    "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "  # Actual detection.\n",
    "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "  # Visualization of the results of a detection.\n",
    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      output_dict['detection_boxes'],\n",
    "      output_dict['detection_classes'],\n",
    "      output_dict['detection_scores'],\n",
    "      category_index,\n",
    "      instance_masks=output_dict.get('detection_masks'),\n",
    "      use_normalized_coordinates=True,\n",
    "      line_thickness=8)\n",
    "  plt.figure(figsize=IMAGE_SIZE)\n",
    "  plt.imshow(image_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
